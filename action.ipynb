{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "# Step 2: Call torch.cuda.empty_cache() to free up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Step 3: Call garbage collector to ensure all unreferenced memory is freed\n",
    "gc.collect()\n",
    "\n",
    "# Optionally, you can also reset the default CUDA stream\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Check if GPU memory is cleared\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available for PyTorch?  True\n",
      "Number of GPUs available:  1\n",
      "GPU  0 :  NVIDIA GeForce RTX 4070\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch GPU\n",
    "print(\"Is CUDA available for PyTorch? \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(\"GPU \", i, \": \", torch.cuda.get_device_name(i))\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "def load_specific_chunk(csv_file_path, start_row, chunk_size):\n",
    "    # Adjust `skiprows` handling here:\n",
    "    # If `start_row` is greater than 0, calculate `skiprows` as before.\n",
    "    # Otherwise, explicitly set `skiprows` to 0 instead of `None`.\n",
    "    skiprows = 1 + start_row if start_row > 0 else 0\n",
    "    \n",
    "    # Now, `skiprows` is guaranteed to be an integer, which should avoid the TypeError.\n",
    "    df_chunk = cudf.read_csv(csv_file_path, skiprows=skiprows, nrows=chunk_size, header=None if skiprows else 'infer')\n",
    "    \n",
    "    return df_chunk\n",
    "\n",
    "# Usage example\n",
    "csv_file_path = 'images_dataset.csv'\n",
    "chunk_size = 100\n",
    "# Load chunks sequentially\n",
    "first_chunk = load_specific_chunk(csv_file_path, 0, chunk_size)\n",
    "df=first_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SELU(inplace=True)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): ConvTranspose2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): SELU(inplace=True)\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): SELU(inplace=True)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): SELU(inplace=True)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "    (16): ConvTranspose2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): SELU(inplace=True)\n",
      "    (19): Dropout(p=0.25, inplace=False)\n",
      "    (20): ConvTranspose2d(256, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (21): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SELU(inplace=True)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): SELU(inplace=True)\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): SELU(inplace=True)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): SELU(inplace=True)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "    (16): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): SELU(inplace=True)\n",
      "    (19): Dropout(p=0.25, inplace=False)\n",
      "    (20): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (21): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3, 16, kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 128, kernel_size=3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 256, kernel_size=3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 3, kernel_size=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 256, kernel_size=4, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(32, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(16, 1, kernel_size=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input if it's flattened\n",
    "        if x.dim() == 2:\n",
    "            x = x.view(x.size(0), 3, 256, 256)\n",
    "        x = self.main(x)\n",
    "        return x.view(x.size(0), -1).mean(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "# Instantiation and summary display\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Assuming you're using GPU for training\n",
    "device = torch.device(\"cuda\")\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "print(generator)\n",
    "print(discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'pixel_0', 'pixel_1', 'pixel_2', 'pixel_3', 'pixel_4',\n",
       "       'pixel_5', 'pixel_6', 'pixel_7', 'pixel_8',\n",
       "       ...\n",
       "       'pixel_196598', 'pixel_196599', 'pixel_196600', 'pixel_196601',\n",
       "       'pixel_196602', 'pixel_196603', 'pixel_196604', 'pixel_196605',\n",
       "       'pixel_196606', 'pixel_196607'],\n",
       "      dtype='object', length=196609)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dt = df.pop('datetime')\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 1\n",
       "1                10\n",
       "2                11\n",
       "3                12\n",
       "4                13\n",
       "          ...      \n",
       "95    20230726_0000\n",
       "96    20230726_0015\n",
       "97    20230726_0030\n",
       "98    20230726_0045\n",
       "99    20230726_0100\n",
       "Name: datetime, Length: 100, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=dt.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After converting and interpolating:\n",
      "0     2023-07-25 08:00:00\n",
      "1     2023-07-25 08:15:00\n",
      "2     2023-07-25 08:30:00\n",
      "3     2023-07-25 08:45:00\n",
      "4     2023-07-25 09:00:00\n",
      "             ...         \n",
      "95    2023-07-26 00:00:00\n",
      "96    2023-07-26 00:15:00\n",
      "97    2023-07-26 00:30:00\n",
      "98    2023-07-26 00:45:00\n",
      "99    2023-07-26 01:00:00\n",
      "Length: 100, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5681/486844610.py:15: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  dt = dt.interpolate()\n",
      "/tmp/ipykernel_5681/486844610.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_range = pd.date_range(start=start_date, periods=len(dt), freq='15T')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure 'dt' is a pandas Series\n",
    "dt = pd.Series(dt)\n",
    "\n",
    "# Convert properly formatted entries\n",
    "mask = dt.str.match(r'\\d{8}_\\d{4}')\n",
    "dt.loc[mask] = pd.to_datetime(dt[mask], format='%Y%m%d_%H%M')\n",
    "\n",
    "# Convert numeric entries to float for interpolation\n",
    "dt.loc[~mask] = pd.to_numeric(dt[~mask], errors='coerce')\n",
    "\n",
    "# Interpolate numeric values\n",
    "dt = dt.interpolate()\n",
    "\n",
    "# Generate a date range for the entire series\n",
    "start_date = dt.loc[mask].min()\n",
    "full_range = pd.date_range(start=start_date, periods=len(dt), freq='15T')\n",
    "\n",
    "# Replace interpolated numeric values with proper datetimes\n",
    "dt.loc[~mask] = full_range[~mask]\n",
    "\n",
    "print(\"After converting and interpolating:\")\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5681/892543047.py:4: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The DataFrame constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
      "  dt_df = pd.DataFrame(dt, columns=['datetime'])\n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "import pandas as pd\n",
    "\n",
    "dt_df = pd.DataFrame(dt, columns=['datetime'])\n",
    "time = cudf.DataFrame.from_pandas(dt_df)\n",
    "df = cudf.concat([df, time], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Move models to the selected device\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# BCE Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizerG = optim.AdamW(generator.parameters(), lr=0.0003)\n",
    "optimizerD = optim.AdamW(discriminator.parameters(), lr=0.0003)\n",
    "\n",
    "# After defining your optimizers\n",
    "scheduler_G = StepLR(optimizerG, step_size=30, gamma=0.1)\n",
    "scheduler_D = StepLR(optimizerD, step_size=30, gamma=0.1)\n",
    "\n",
    "# Learning rate scheduler\n",
    "schedulerG = optim.lr_scheduler.ReduceLROnPlateau(optimizerG, patience=5, factor=0.1)\n",
    "schedulerD = optim.lr_scheduler.ReduceLROnPlateau(optimizerD, patience=5, factor=0.1)\n",
    "\n",
    "num_epochs = 1  # For testing purposes\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# Assuming 'df' is your cuDF DataFrame\n",
    "# Calculate total number of rows\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate split indices\n",
    "train_end = int(total_rows * 0.8)\n",
    "validate_end = train_end + int(total_rows * 0.15)\n",
    "\n",
    "# Split the DataFrame into train, validate, and test sets\n",
    "train_df = df.iloc[:train_end]\n",
    "validate_df = df.iloc[train_end:validate_end]\n",
    "test_df = df.iloc[validate_end:]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.to_pandas()\n",
    "validate_df = validate_df.to_pandas()\n",
    "test_df = test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)  # Reset index to ensure it starts from 0\n",
    "        self.pixel_columns = [col for col in dataframe.columns if col.startswith('pixel_')]\n",
    "        self.datetime_column = 'datetime'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pixel_data = self.dataframe.iloc[idx][self.pixel_columns].values.astype('float32')\n",
    "        datetime_data = pd.to_datetime(self.dataframe.iloc[idx][self.datetime_column]).timestamp()\n",
    "        \n",
    "        pixel_tensor = torch.tensor(pixel_data, dtype=torch.float32)\n",
    "        datetime_tensor = torch.tensor(datetime_data, dtype=torch.float32)\n",
    "        \n",
    "        return pixel_tensor, datetime_tensor\n",
    "\n",
    "# Assuming train_df, validate_df, and test_df are your DataFrames\n",
    "train_dataset = ImageDataset(train_df)\n",
    "validate_dataset = ImageDataset(validate_df)\n",
    "test_dataset = ImageDataset(test_df)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Pixel data:\n",
      "tensor([[0.2431, 0.2980, 0.2314,  ..., 0.7412, 0.7608, 0.7373],\n",
      "        [0.1333, 0.1333, 0.1333,  ..., 0.6471, 0.6471, 0.6471],\n",
      "        [0.1647, 0.1647, 0.1647,  ..., 0.6275, 0.6275, 0.6275],\n",
      "        ...,\n",
      "        [0.1569, 0.1569, 0.1569,  ..., 0.4784, 0.4784, 0.4784],\n",
      "        [0.1216, 0.1216, 0.1216,  ..., 0.3843, 0.3843, 0.3843],\n",
      "        [0.1843, 0.1843, 0.1843,  ..., 0.3647, 0.3647, 0.3647]])\n",
      "Pixel data size: torch.Size([8, 196608])\n",
      "\n",
      "Datetime data:\n",
      "tensor([1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09,\n",
      "        1.6903e+09, 1.6903e+09])\n",
      "Datetime data size: torch.Size([8])\n",
      "\n",
      "==================================================\n",
      "\n",
      "Batch 2:\n",
      "Pixel data:\n",
      "tensor([[0.2941, 0.2941, 0.2941,  ..., 0.5412, 0.5412, 0.5412],\n",
      "        [0.2157, 0.2157, 0.2157,  ..., 0.7686, 0.7686, 0.7686],\n",
      "        [0.1922, 0.2471, 0.1804,  ..., 0.7294, 0.7686, 0.7216],\n",
      "        ...,\n",
      "        [0.1451, 0.2000, 0.1333,  ..., 0.7137, 0.7333, 0.7098],\n",
      "        [0.1333, 0.1725, 0.1294,  ..., 0.7216, 0.7412, 0.7176],\n",
      "        [0.0980, 0.0980, 0.0980,  ..., 0.7725, 0.7725, 0.7725]])\n",
      "Pixel data size: torch.Size([8, 196608])\n",
      "\n",
      "Datetime data:\n",
      "tensor([1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09,\n",
      "        1.6903e+09, 1.6903e+09])\n",
      "Datetime data size: torch.Size([8])\n",
      "\n",
      "==================================================\n",
      "\n",
      "Batch 3:\n",
      "Pixel data:\n",
      "tensor([[0.0745, 0.1137, 0.0824,  ..., 0.8118, 0.8118, 0.8118],\n",
      "        [0.0706, 0.0706, 0.0706,  ..., 0.8078, 0.8078, 0.8078],\n",
      "        [0.0588, 0.0588, 0.0588,  ..., 0.8118, 0.8118, 0.8118],\n",
      "        ...,\n",
      "        [0.0275, 0.0275, 0.0275,  ..., 0.7098, 0.7294, 0.7451],\n",
      "        [0.0510, 0.0510, 0.0510,  ..., 0.7686, 0.7882, 0.8353],\n",
      "        [0.0588, 0.0588, 0.0588,  ..., 0.9412, 0.9255, 0.8941]])\n",
      "Pixel data size: torch.Size([8, 196608])\n",
      "\n",
      "Datetime data:\n",
      "tensor([1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09,\n",
      "        1.6903e+09, 1.6903e+09])\n",
      "Datetime data size: torch.Size([8])\n",
      "\n",
      "==================================================\n",
      "\n",
      "Batch 4:\n",
      "Pixel data:\n",
      "tensor([[0.0549, 0.0549, 0.0549,  ..., 0.8196, 0.8000, 0.8314],\n",
      "        [0.0471, 0.0471, 0.0471,  ..., 0.7216, 0.7373, 0.8275],\n",
      "        [0.0510, 0.0510, 0.0510,  ..., 0.6667, 0.7529, 0.8431],\n",
      "        ...,\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.6706, 0.6902, 0.8196],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.7961, 0.8627, 0.8941],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.8980, 0.8863, 0.8941]])\n",
      "Pixel data size: torch.Size([8, 196608])\n",
      "\n",
      "Datetime data:\n",
      "tensor([1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09, 1.6903e+09,\n",
      "        1.6903e+09, 1.6903e+09])\n",
      "Datetime data size: torch.Size([8])\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_counter = 0\n",
    "num_batches_to_print = 4\n",
    "\n",
    "for pixel_batch, datetime_batch in train_loader:\n",
    "    print(f\"Batch {batch_counter + 1}:\")\n",
    "    print(\"Pixel data:\")\n",
    "    print(pixel_batch)\n",
    "    print(f\"Pixel data size: {pixel_batch.size()}\")\n",
    "    print(\"\\nDatetime data:\")\n",
    "    print(datetime_batch)\n",
    "    print(f\"Datetime data size: {datetime_batch.size()}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    batch_counter += 1\n",
    "    \n",
    "    if batch_counter >= num_batches_to_print:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 510.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m real_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m criterion(output, real_labels)\n\u001b[1;32m     18\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     76\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 510.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for i, (real_data, datetime_data) in enumerate(train_loader):\n",
    "        batch_size = real_data.size(0)\n",
    "        real_data = real_data.to(device)\n",
    "        datetime_data = datetime_data.unsqueeze(1).float().to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        output = discriminator(real_data)\n",
    "        d_loss_real = criterion(output, real_labels)\n",
    "        \n",
    "        noise = torch.randn(batch_size, 3, 256, 256, device=device)\n",
    "        fake_data = generator(noise)\n",
    "        output = discriminator(fake_data.detach())\n",
    "        d_loss_fake = criterion(output, fake_labels)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "        scheduler_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        generator.zero_grad()\n",
    "        output = discriminator(fake_data)\n",
    "        g_loss = criterion(output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        scheduler_G.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], '\n",
    "                  f'd_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    val_d_loss = 0.0\n",
    "    val_g_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for real_data, datetime_data in validate_loader:\n",
    "            real_data = real_data.to(device)\n",
    "            datetime_data = datetime_data.unsqueeze(1).float().to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            output = discriminator(real_data)\n",
    "            d_loss_real = criterion(output, real_labels)\n",
    "            \n",
    "            noise = torch.randn(batch_size, 3, 256, 256, device=device)\n",
    "            fake_data = generator(noise)\n",
    "            output = discriminator(fake_data)\n",
    "            d_loss_fake = criterion(output, fake_labels)\n",
    "            \n",
    "            val_d_loss += (d_loss_real + d_loss_fake).item()\n",
    "            \n",
    "            output = discriminator(fake_data)\n",
    "            g_loss = criterion(output, real_labels)\n",
    "            \n",
    "            val_g_loss += g_loss.item()\n",
    "    \n",
    "    val_d_loss /= len(validate_loader)\n",
    "    val_g_loss /= len(validate_loader)\n",
    "    \n",
    "    schedulerG.step(val_g_loss)\n",
    "    schedulerD.step(val_d_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation D Loss: {val_d_loss:.4f}, Validation G Loss: {val_g_loss:.4f}\")\n",
    "\n",
    "    # Early stopping and model saving\n",
    "    if val_g_loss < best_loss:\n",
    "        best_loss = val_g_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(generator.state_dict(), 'best_generator.pth')\n",
    "        torch.save(discriminator.state_dict(), 'best_discriminator.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Test phase\n",
    "generator.load_state_dict(torch.load('best_generator.pth'))\n",
    "discriminator.load_state_dict(torch.load('best_discriminator.pth'))\n",
    "\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "test_d_loss = 0.0\n",
    "test_g_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for real_data, datetime_data in test_loader:\n",
    "        real_data = real_data.to(device)\n",
    "        datetime_data = datetime_data.unsqueeze(1).float().to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "        \n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        output = discriminator(real_data)\n",
    "        d_loss_real = criterion(output, real_labels)\n",
    "        \n",
    "        noise = torch.randn(batch_size, 3, 256, 256, device=device)\n",
    "        fake_data = generator(noise)\n",
    "        output = discriminator(fake_data)\n",
    "        d_loss_fake = criterion(output, fake_labels)\n",
    "        \n",
    "        test_d_loss += (d_loss_real + d_loss_fake).item()\n",
    "        \n",
    "        output = discriminator(fake_data)\n",
    "        g_loss = criterion(output, real_labels)\n",
    "        \n",
    "        test_g_loss += g_loss.item()\n",
    "\n",
    "test_d_loss /= len(test_loader)\n",
    "test_g_loss /= len(test_loader)\n",
    "\n",
    "print(f\"Test D Loss: {test_d_loss:.4f}, Test G Loss: {test_g_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalising everything  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
